{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1b16qeSkKvp0H6sBVJr1Ad4YuaFRueFOC",
      "authorship_tag": "ABX9TyMkAM2fEcLk9uII4vTzlBU+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyunWrites0721/LLM_Based_OTT_recommender/blob/main/LLM_Based_OTTRec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GTM_LJdQuDDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96cf562a-3b7b-4c14-cd60-c03fe5a5dad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\n",
            "ìœ ì € ìˆ˜: 6040, ì˜í™” ìˆ˜: 3883, í‰ì  ìˆ˜: 1000209\n"
          ]
        }
      ],
      "source": [
        "#ë°ì´í„° ë¡œë”©.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/for_data/ml-1m/\"\n",
        "\n",
        "# 1. Ratings: ìœ ì €ì˜ ì˜í™” í‰ì  ë°ì´í„°\n",
        "ratings = pd.read_csv(path + 'ratings.dat', sep='::', engine='python', encoding='latin-1',\n",
        "                      names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
        "\n",
        "# 2. Movies: ì˜í™” ì œëª© ë° ì¥ë¥´ ë°ì´í„°\n",
        "movies = pd.read_csv(path + 'movies.dat', sep='::', engine='python', encoding='latin-1',\n",
        "                     names=['movie_id', 'title', 'genres'])\n",
        "\n",
        "# 3. Users: ìœ ì € ë°ëª¨ê·¸ë˜í”½(ì„±ë³„, ë‚˜ì´ ë“±) ë°ì´í„°\n",
        "users = pd.read_csv(path + 'users.dat', sep='::', engine='python', encoding='latin-1',\n",
        "                    names=['user_id', 'gender', 'age', 'occupation', 'zip_code'])\n",
        "\n",
        "print(\"ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"ìœ ì € ìˆ˜: {len(users)}, ì˜í™” ìˆ˜: {len(movies)}, í‰ì  ìˆ˜: {len(ratings)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ë°ì´í„° ì •ì œ\n",
        "\n",
        "# 1. ì œëª©(Title)ì„ ê¸°ì¤€ìœ¼ë¡œ ê³ ìœ í•œ ì˜í™” ëª©ë¡ ìƒì„±\n",
        "unique_movies = movies.drop_duplicates(subset=['title']).copy()\n",
        "\n",
        "# 2. ìƒˆë¡œìš´ ê³ ìœ  ID ë¶€ì—¬ (1ë¶€í„° ì‹œì‘, íŒ¨ë”©ì´ 0ì´ë¯€ë¡œ 1ë¶€í„° ì‹œì‘í•´ì•¼ í•¨.)\n",
        "unique_movies['new_movie_id'] = range(1, len(unique_movies)+1)\n",
        "\n",
        "# 3. {ê¸°ì¡´ ID : ìƒˆë¡œìš´ ID} ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "id_mapping = pd.merge(movies, unique_movies[['title', 'new_movie_id']], on='title')\n",
        "mapping_dict = dict(zip(id_mapping['movie_id'], id_mapping['new_movie_id']))\n",
        "\n",
        "# 4. Ratings ë°ì´í„° ê¸°ì¡´ IDë¥¼ ìƒˆë¡œìš´ ê³ ìœ  IDë¡œ ì¹˜í™˜ (ë§¤í•‘ë˜ì§€ ì•ŠëŠ” ìœ ë ¹ IDëŠ” ì œê±°)\n",
        "ratings['movie_id'] = ratings['movie_id'].map(mapping_dict)\n",
        "ratings = ratings.dropna(subset=['movie_id']).astype({'movie_id': int})\n",
        "\n",
        "# 5. ìµœì¢… ê²°ê³¼ í™•ì¸\n",
        "print(f\"ê¸°ì¡´ ì˜í™” ìˆ˜: {len(movies)}\")\n",
        "print(f\"ì •ì œ í›„ ê³ ìœ  ì˜í™” ìˆ˜: {len(unique_movies)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywc5RUGvviSc",
        "outputId": "d38c9447-ad6e-4160-d3b6-096e1bb3a97e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê¸°ì¡´ ì˜í™” ìˆ˜: 3883\n",
            "ì •ì œ í›„ ê³ ìœ  ì˜í™” ìˆ˜: 3883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(unique_movies)\n",
        "print(ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NECCwL9C8ccX",
        "outputId": "e292891b-1641-4122-c597-cd9197d779e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      movie_id                               title  \\\n",
            "0            1                    Toy Story (1995)   \n",
            "1            2                      Jumanji (1995)   \n",
            "2            3             Grumpier Old Men (1995)   \n",
            "3            4            Waiting to Exhale (1995)   \n",
            "4            5  Father of the Bride Part II (1995)   \n",
            "...        ...                                 ...   \n",
            "3878      3948             Meet the Parents (2000)   \n",
            "3879      3949          Requiem for a Dream (2000)   \n",
            "3880      3950                    Tigerland (2000)   \n",
            "3881      3951             Two Family House (2000)   \n",
            "3882      3952               Contender, The (2000)   \n",
            "\n",
            "                            genres  new_movie_id  \n",
            "0      Animation|Children's|Comedy             1  \n",
            "1     Adventure|Children's|Fantasy             2  \n",
            "2                   Comedy|Romance             3  \n",
            "3                     Comedy|Drama             4  \n",
            "4                           Comedy             5  \n",
            "...                            ...           ...  \n",
            "3878                        Comedy          3879  \n",
            "3879                         Drama          3880  \n",
            "3880                         Drama          3881  \n",
            "3881                         Drama          3882  \n",
            "3882                Drama|Thriller          3883  \n",
            "\n",
            "[3883 rows x 4 columns]\n",
            "         user_id  movie_id  rating  timestamp\n",
            "0              1      1177       5  978300760\n",
            "1              1       656       3  978302109\n",
            "2              1       903       3  978301968\n",
            "3              1      3340       4  978300275\n",
            "4              1      2287       5  978824291\n",
            "...          ...       ...     ...        ...\n",
            "1000204     6040      1076       1  956716541\n",
            "1000205     6040      1079       5  956704887\n",
            "1000206     6040       559       5  956704746\n",
            "1000207     6040      1081       4  956715648\n",
            "1000208     6040      1082       4  956715569\n",
            "\n",
            "[1000209 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ratingsì— ë°”ë€ IDë“¤ì´ ì˜ ë“¤ì–´ê°”ëŠ”ì§€ í™•ì¸\n",
        "print(f\"ìµœëŒ€ ì˜í™” ID: {ratings['movie_id'].max()}\") # 3883ì´ ë‚˜ì™€ì•¼ í•¨\n",
        "print(f\"ìµœì†Œ ì˜í™” ID: {ratings['movie_id'].min()}\") # 1ì´ ë‚˜ì™€ì•¼ í•¨\n",
        "\n",
        "# 2. item_count ì¬ì„¤ì •\n",
        "item_count = ratings['movie_id'].max() + 1 # 3884ê°€ ë˜ì–´ì•¼ í•¨ (0ë²ˆ íŒ¨ë”© í¬í•¨)\n",
        "\n",
        "item_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjOsAR4bm1pz",
        "outputId": "b0bd515a-7f2c-4f26-91e6-7c1915b6b34e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìµœëŒ€ ì˜í™” ID: 3883\n",
            "ìµœì†Œ ì˜í™” ID: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3884"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SASRec í•™ìŠµìš© ìœ ì € ì‹œí€€ìŠ¤ ë§Œë“¤ê¸°\n",
        "\n",
        "# ìœ ì €ë³„ë¡œ ì˜í™” ì‹œì²­ ì´ë ¥ì„ ì‹œê°„ìˆœìœ¼ë¡œ ë¬¶ê¸°\n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "user_test = {}\n",
        "\n",
        "user_group = ratings.groupby('user_id')\n",
        "#print(user_group.groups)\n",
        "\n",
        "for user, group in user_group:\n",
        "    seq = group.sort_values('timestamp')['movie_id'].tolist()\n",
        "\n",
        "    # ë°ì´í„°ê°€ ë„ˆë¬´ ì ì€ ìœ ì €ëŠ” ì œì™¸ (ìµœì†Œ 3ê°œ ì´ìƒ, train/valid/testë¡œ ë¶„í• í•˜ê¸° ìœ„í•´)\n",
        "    if len(seq) < 3:\n",
        "        continue\n",
        "\n",
        "    # Leave-one-out ë¶„í• \n",
        "    user_train[user] = seq[:-2]\n",
        "    user_valid[user] = [seq[-2]]\n",
        "    user_test[user] = [seq[-1]]\n",
        "\n",
        "print(f\"ë‚¨ì€ ìœ ì € ìˆ˜: {len(user_train)}\")\n",
        "print(f\"1ë²ˆ ìœ ì €ì˜ í•™ìŠµ ì‹œí€€ìŠ¤ ì˜ˆì‹œ: {user_train[1][:5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8IwFixvAjpR",
        "outputId": "00187ad4-7647-46a7-db1a-b8897018a448"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‚¨ì€ ìœ ì € ìˆ˜: 6040\n",
            "1ë²ˆ ìœ ì €ì˜ í•™ìŠµ ì‹œí€€ìŠ¤ ì˜ˆì‹œ: [3118, 1251, 1673, 1010, 2272]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê°€ìš© ë©”íƒ€ë°ì´í„°1 : total_users, all_item_counts, item_genre_dict\n",
        "\n",
        "# 1. total_users: ì „ì²´ ìœ ì € ìˆ˜ ê³„ì‚°\n",
        "total_users = ratings['user_id'].nunique()\n",
        "\n",
        "# 2. all_item_counts: ì•„ì´í…œë³„ ë…¸ì¶œ ë¹ˆë„ (Novelty ê³„ì‚°ìš©)\n",
        "# ê° ì˜í™”ê°€ ëª‡ ëª…ì˜ ìœ ì €ì—ê²Œ ì†Œë¹„ë˜ì—ˆëŠ”ì§€ ê³„ì‚°\n",
        "all_item_counts = ratings['movie_id'].value_counts().to_dict()\n",
        "\n",
        "# 3. item_genre_dict: {new_movie_id: [ì¥ë¥´ ë¦¬ìŠ¤íŠ¸]} (Diversity, Transitionìš©)\n",
        "item_genre_dict = {}\n",
        "\n",
        "for _, row in unique_movies.iterrows():\n",
        "    new_id = row['new_movie_id']\n",
        "    # ì¥ë¥´ ë°ì´í„°ê°€ 'Action|Sci-Fi' í˜•íƒœì¼ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í• \n",
        "    genres = row['genres'].split('|')\n",
        "    item_genre_dict[new_id] = genres\n",
        "\n",
        "#print(f\"ì „ì²´ ìœ ì € ìˆ˜: {total_users}\")\n",
        "#print(f\"ì¥ë¥´ ì •ë³´ê°€ í¬í•¨ëœ ì˜í™” ìˆ˜: {len(item_genre_dict)}\")\n",
        "#print(f\"1ë²ˆ ì˜í™”ì˜ ì¥ë¥´: {item_genre_dict.get(1)}\")\n",
        "\n",
        "\n",
        "#í‰ê°€ìš© ë©”íƒ€ë°ì´í„°2 : all_genres, genre_to_idx, idx_to_genre\n",
        "\n",
        "# ëª¨ë“  ê³ ìœ  ì¥ë¥´ ì¶”ì¶œ ë° ì •ë ¬\n",
        "all_genres = sorted(list(set([g for genres in item_genre_dict.values() for g in genres])))\n",
        "\n",
        "# ì¥ë¥´ëª… <-> í–‰ë ¬ ì¸ë±ìŠ¤ ë§¤í•‘\n",
        "genre_to_idx = {g: i for i, g in enumerate(all_genres)}\n",
        "idx_to_genre = {i: g for i, g in enumerate(all_genres)}\n",
        "\n",
        "#print(f\"ì¶”ì¶œëœ ì´ ì¥ë¥´ ìˆ˜: {len(all_genres)}\")\n",
        "#print(f\"ì¥ë¥´ ë§¤í•‘ ì˜ˆì‹œ: {list(genre_to_idx.items())[:3]}\")"
      ],
      "metadata": {
        "id": "MwYWttBLr4r3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í‰ê°€ìš© ë©”íƒ€ë°ì´í„°3: ì¥ë¥´ ì „ì´ í–‰ë ¬ ê³„ì‚° í•¨ìˆ˜: í•œ ì¥ë¥´ì—ì„œ ë‹¤ë¥¸ ì¥ë¥´ë¡œ ë„˜ì–´ê°ˆ í™•ë¥ ì„ ì €ì¥í•˜ëŠ” í–‰ë ¬\n",
        "\n",
        "def build_genre_transition_matrix(ratings, item_genre_dict):\n",
        "    # ëª¨ë“  ê³ ìœ  ì¥ë¥´ ì¶”ì¶œ\n",
        "    all_genres = sorted(list(set([g for genres in item_genre_dict.values() for g in genres])))\n",
        "    genre_to_idx = {g: i for i, g in enumerate(all_genres)}\n",
        "    num_genres = len(all_genres)\n",
        "\n",
        "    # matrix[from_genre][to_genre]\n",
        "    matrix = np.zeros((num_genres, num_genres))\n",
        "\n",
        "    # ìœ ì €ë³„ ì‹œí€€ìŠ¤ í™•ì¸\n",
        "    for user, group in ratings.groupby('user_id'):\n",
        "        seq = group.sort_values('timestamp')['movie_id'].tolist()\n",
        "        for i in range(len(seq) - 1):\n",
        "            from_genres = item_genre_dict.get(seq[i], [])\n",
        "            to_genres = item_genre_dict.get(seq[i+1], [])\n",
        "\n",
        "            # ë‹¤ì¤‘ ì¥ë¥´ì¼ ê²½ìš° ëª¨ë“  ì¡°í•©ì— ëŒ€í•´ ë¹ˆë„ ì¶”ê°€ (ë¶„ì‚° ì²˜ë¦¬)\n",
        "            for fg in from_genres:\n",
        "                for tg in to_genres:\n",
        "                    matrix[genre_to_idx[fg]][genre_to_idx[tg]] += 1\n",
        "\n",
        "    # í–‰ ë‹¨ìœ„ë¡œ ì •ê·œí™” (í™•ë¥ ë¡œ ë³€í™˜)\n",
        "    row_sums = matrix.sum(axis=1)\n",
        "    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€\n",
        "    transition_matrix = np.divide(matrix, row_sums[:, np.newaxis],\n",
        "                                  where=row_sums[:, np.newaxis]!=0)\n",
        "\n",
        "    return transition_matrix, genre_to_idx\n",
        "\n",
        "# ì‹¤í–‰\n",
        "transition_matrix, genre_mapping = build_genre_transition_matrix(ratings, item_genre_dict)"
      ],
      "metadata": {
        "id": "lFY6XjuhoAQF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SASRecDataset í´ë˜ìŠ¤ ì •ì˜\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class SASRecDataset(Dataset):\n",
        "    def __init__(self, user_train, item_count, max_len):\n",
        "        self.user_train = user_train  # {user ID: [item list]} dictionary\n",
        "        self.item_count = item_count  # total number of movies\n",
        "        self.max_len = max_len        # maximum sequence length L\n",
        "        self.user_list = list(user_train.keys())  # List of user ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.user_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        user = self.user_list[index]\n",
        "        seq = self.user_train[user]\n",
        "\n",
        "        # 1. Sequence Padding & Truncating\n",
        "        # Pad by 0 or Truncate if larger than L\n",
        "        input_seq = np.zeros([self.max_len], dtype=np.int32)\n",
        "        pos = np.zeros([self.max_len], dtype=np.int32)  #true next item\n",
        "        neg = np.zeros([self.max_len], dtype=np.int32)  #not next item\n",
        "\n",
        "        # nxtëŠ” ì •ë‹µ(Positive), idxëŠ” í˜„ì¬ ì…ë ¥(Input)\n",
        "        nxt = seq[-1]\n",
        "        idx = self.max_len - 1\n",
        "\n",
        "        # ë’¤ì—ì„œë¶€í„° ì±„ì›Œë‚˜ê°€ëŠ” ë°©ì‹ (Left Paddingì„ ìœ„í•¨)\n",
        "        for i in reversed(seq[:-1]):\n",
        "            input_seq[idx] = i\n",
        "            pos[idx] = nxt\n",
        "            # Negative Sampling: random sampling from items user haven't seen\n",
        "            if nxt != 0: # if not padding\n",
        "                neg[idx] = self._get_neg_item(seq)\n",
        "            nxt = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "\n",
        "        return torch.LongTensor([user]), torch.LongTensor(input_seq), \\\n",
        "               torch.LongTensor(pos), torch.LongTensor(neg)\n",
        "\n",
        "    def _get_neg_item(self, seq):\n",
        "        # í•™ìŠµ íš¨ìœ¨ì„ ìœ„í•´ ìœ ì € ì‹œí€€ìŠ¤ì— ì—†ëŠ” ì•„ì´í…œì„ ëœë¤í•˜ê²Œ ë½‘ìŒ\n",
        "        t = np.random.randint(1, self.item_count)\n",
        "        while t in seq:\n",
        "            t = np.random.randint(1, self.item_count)\n",
        "        return t"
      ],
      "metadata": {
        "id": "Y-V4sNWxnWMi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í‰ê°€ìš© ë‹¨ê¸°ì§€í‘œ ê³„ì‚° í•¨ìˆ˜\n",
        "#ë°˜í™˜ê°’: Recall, NDCG\n",
        "\n",
        "import math\n",
        "\n",
        "def evaluate_metrics(predictions, ground_truth, k_list=[5, 10]):\n",
        "    \"\"\"\n",
        "    predictions: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì „ì²´ ì•„ì´í…œì— ëŒ€í•œ ì ìˆ˜ (Batch, Item_Count)\n",
        "    ground_truth: ì‹¤ì œ ì •ë‹µ ì•„ì´í…œ ID (Batch, 1)\n",
        "    k_list: ê³„ì‚°í•˜ê³  ì‹¶ì€ K ê°’ì˜ ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # 1. ìƒìœ„ Kê°œì˜ ì•„ì´í…œ ì¸ë±ìŠ¤(ID) ì¶”ì¶œ\n",
        "    # torch.topkëŠ” ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ê°€ì¥ ë†’ì€ ê°’ kê°œë¥¼ ë°˜í™˜í•¨\n",
        "    max_k = max(k_list)\n",
        "    _, top_indices = torch.topk(predictions, max_k, dim=-1) # (Batch, max_k)\n",
        "\n",
        "    # 2. ê° Kë³„ë¡œ ì§€í‘œ ê³„ì‚°\n",
        "    for k in k_list:\n",
        "        hit = 0.0\n",
        "        ndcg = 0.0\n",
        "\n",
        "        for i in range(len(ground_truth)):\n",
        "            target = ground_truth[i]       # ì‹¤ì œ ì •ë‹µ ID\n",
        "            recommended = top_indices[i][:k] # ìƒìœ„ kê°œ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "            if target in recommended:\n",
        "                # [Recall ê³„ì‚°] í¬í•¨ë˜ë©´ 1 ì•„ë‹ˆë©´ 0\n",
        "                hit += 1.0\n",
        "\n",
        "                # [NDCG ê³„ì‚°] ì •ë‹µì˜ ìˆœìœ„(rank)ë¥¼ ì°¾ì•„ ë¡œê·¸ ê°ì‡  ì ìš©\n",
        "                # indexëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ +1ì„ í•´ì¤Œ, ë˜ +1ì„ í•˜ëŠ” ì´ìœ ëŠ” log ë•Œë¬¸ì—. ê·¸ë˜ì„œ +2.\n",
        "                rank = (recommended == target).nonzero(as_tuple=True)[0].item()\n",
        "                ndcg += 1.0 / math.log2(rank + 2)\n",
        "\n",
        "        results[f'Recall@{k}'] = hit / len(ground_truth)\n",
        "        results[f'NDCG@{k}'] = ndcg / len(ground_truth)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ntEbcZ90rh5u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í‰ê°€ìš© ì¥ê¸°ì§€í‘œ ê³„ì‚° í•¨ìˆ˜\n",
        "#Intra-List Diversity / Novelty / Serendipity / Genre-Transition Probability / Recency-Weighted Satisfaction\n",
        "\n",
        "def evaluate_long_term_metrics(recommended_list, user_history, ground_truth, item_genre_dict,\n",
        "                               all_item_counts, total_users, transition_matrix, genre_to_idx):\n",
        "    \"\"\"\n",
        "    ground_truth: ì‹¤ì œ ìœ ì €ê°€ ë³¸ ë‹¤ìŒ ì˜í™” ID (ë³´í†µ user_test[user][0])\n",
        "    transition_matrix: build_genre_transition_matrix í•¨ìˆ˜ë¡œ ë§Œë“  í–‰ë ¬\n",
        "    genre_to_idx: ì¥ë¥´ëª… -> í–‰ë ¬ ì¸ë±ìŠ¤ ë§¤í•‘ ì‚¬ì „\n",
        "    \"\"\"\n",
        "\n",
        "    # ê³µí†µ í•¨ìˆ˜: Jaccard ìœ ì‚¬ë„\n",
        "    def get_genre_sim(item1, item2):\n",
        "        g1, g2 = set(item_genre_dict.get(item1, [])), set(item_genre_dict.get(item2, []))\n",
        "        if not g1 or not g2: return 0\n",
        "        return len(g1 & g2) / len(g1 | g2)\n",
        "\n",
        "    # 1. Intra-List Diversity\n",
        "    sims = []\n",
        "    for i in range(len(recommended_list)):\n",
        "        for j in range(i + 1, len(recommended_list)):\n",
        "            sims.append(get_genre_sim(recommended_list[i], recommended_list[j]))\n",
        "    intra_diversity = 1 - np.mean(sims) if sims else 0\n",
        "\n",
        "    # 2. Novelty\n",
        "    novelty = 0\n",
        "    for item in recommended_list:\n",
        "        prob = (all_item_counts.get(item, 0)+1) / (total_users + 1)\n",
        "        novelty -= np.log2(prob)\n",
        "    novelty /= len(recommended_list)\n",
        "\n",
        "    # 3. Serendipity\n",
        "    # ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ì— ì •ë‹µ(ground_truth)ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•˜ë©°, ê·¸ ì •ë‹µì´ ê³¼ê±° íˆìŠ¤í† ë¦¬ ì•„ì´í…œë“¤ê³¼ì˜ í‰ê·  ìœ ì‚¬ë„ê°€ ë‚®ì•„ì•¼ í•¨.\n",
        "    serendipity = 0\n",
        "    if ground_truth in recommended_list:\n",
        "        # ì •ë‹µ ì•„ì´í…œê³¼ ê³¼ê±° íˆìŠ¤í† ë¦¬ ê°„ì˜ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        rel_sims = [get_genre_sim(ground_truth, hist_item) for hist_item in user_history]\n",
        "        avg_rel_sim = np.mean(rel_sims) if rel_sims else 0\n",
        "        # ìœ ì‚¬ë„ê°€ ë‚®ì„ìˆ˜ë¡(ì¦‰, ì˜ì™¸ì¼ìˆ˜ë¡) ë†’ì€ ì ìˆ˜\n",
        "        serendipity = 1 - avg_rel_sim\n",
        "\n",
        "    # 4. Genre-Transition Probability (ì „ì´ í–‰ë ¬ í™œìš©)\n",
        "    # ë§ˆì§€ë§‰ ì•„ì´í…œì˜ ì¥ë¥´ë“¤ë¡œë¶€í„° ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œë“¤ì˜ ì¥ë¥´ë“¤ë¡œ ì´ë™í•  í™•ë¥ ì˜ í‰ê· \n",
        "    from_genres = item_genre_dict.get(user_history[-1], [])\n",
        "    transition_scores = []\n",
        "\n",
        "    for rec_item in recommended_list:\n",
        "        to_genres = item_genre_dict.get(rec_item, [])\n",
        "        item_transition_probs = []\n",
        "        for fg in from_genres:\n",
        "            for tg in to_genres:\n",
        "                # í–‰ë ¬ì—ì„œ í™•ë¥ ê°’ ì¶”ì¶œ\n",
        "                f_idx, t_idx = genre_to_idx[fg], genre_to_idx[tg]\n",
        "                item_transition_probs.append(transition_matrix[f_idx, t_idx])\n",
        "\n",
        "        if item_transition_probs:\n",
        "            transition_scores.append(np.mean(item_transition_probs))\n",
        "\n",
        "    genre_transition = np.mean(transition_scores) if transition_scores else 0\n",
        "\n",
        "    # 5. Recency-Weighted Satisfaction\n",
        "    recency_weighted_sim = 0\n",
        "    lambda_decay = 0.1\n",
        "    for idx, hist_item in enumerate(reversed(user_history)):\n",
        "        weight = np.exp(-lambda_decay * idx)\n",
        "        for rec_item in recommended_list:\n",
        "            recency_weighted_sim += weight * get_genre_sim(hist_item, rec_item)\n",
        "\n",
        "    return {\n",
        "        \"Diversity\": intra_diversity,\n",
        "        \"Novelty\": novelty,\n",
        "        \"Serendipity\": serendipity,\n",
        "        \"Transition_Prob\": genre_transition,\n",
        "        \"Recency_Satisfy\": recency_weighted_sim\n",
        "    }"
      ],
      "metadata": {
        "id": "gh4iedTSkMJ_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ëª¨ë¸ êµ¬ì¡° ì •ì˜ : class SASRec(nn.Module)\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "    def __init__(self, item_count, hidden_units, num_blocks, num_heads, max_len, dropout_rate, device):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.item_count = item_count\n",
        "        self.device = device\n",
        "\n",
        "        # 1. Embedding Layers\n",
        "        # item_count + 1 ì¸ ì´ìœ ëŠ” 0ë²ˆ íŒ¨ë”© ë•Œë¬¸\n",
        "        self.item_emb = nn.Embedding(item_count, hidden_units, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len + 1, hidden_units)\n",
        "        self.emb_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # 2. Multi-head Attention Blocks\n",
        "        self.attention_blocks = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=hidden_units,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=hidden_units,\n",
        "                dropout=dropout_rate,\n",
        "                activation='relu',\n",
        "                batch_first=True\n",
        "            ) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.last_layernorm = nn.LayerNorm(hidden_units)\n",
        "\n",
        "    def forward(self, log_seqs):\n",
        "        seqs = self.item_emb(log_seqs)\n",
        "        positions = torch.arange(log_seqs.shape[1], device=self.device).unsqueeze(0)\n",
        "        seqs += self.pos_emb(positions)\n",
        "        seqs = self.emb_dropout(seqs)\n",
        "\n",
        "        # 1. íŒ¨ë”© ë§ˆìŠ¤í¬ (bool íƒ€ì…)\n",
        "        timeline_mask = (log_seqs == 0) # (Batch, max_len)\n",
        "\n",
        "        # 2. ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨ ë§ˆìŠ¤í¬ (bool íƒ€ì…ì´ ê°€ì¥ ì•ˆì •ì ì…ë‹ˆë‹¤)\n",
        "        L = log_seqs.shape[1]\n",
        "        # ìƒì‚¼ê° í–‰ë ¬ì„ ìƒì„±í•˜ì—¬ Trueì¸ ë¶€ë¶„ì„ ê°€ë¦¼\n",
        "        attn_mask = torch.triu(torch.ones(L, L, device=self.device), diagonal=1).to(torch.bool)\n",
        "\n",
        "        for block in self.attention_blocks:\n",
        "            # src_maskì™€ src_key_padding_maskì˜ íƒ€ì…ì„ ë§ì¶°ì¤Œ\n",
        "            seqs = block(seqs, src_mask=attn_mask, src_key_padding_mask=timeline_mask)\n",
        "\n",
        "        log_feats = self.last_layernorm(seqs)\n",
        "\n",
        "        return log_feats\n",
        "\n",
        "    def predict(self, log_feats):\n",
        "        \"\"\"\n",
        "        log_feats: (Batch, Max_Len, Hidden_Units) - model's result(recommendation)\n",
        "        \"\"\"\n",
        "        # 1. ëª¨ë“  ì•„ì´í…œ ì„ë² ë”©ì„ ê°€ì ¸ì˜´ (Item_Count + 1, Hidden_Units)\n",
        "        item_embs = self.item_emb.weight\n",
        "\n",
        "        # 2. ìœ ì €ì˜ ë§ˆì§€ë§‰ ìƒíƒœ(ì·¨í–¥)ì„ ì¶”ì¶œ (Batch, Hidden_Units)\n",
        "        # log_feats's last time step [-1] is user's current(final) taste\n",
        "        final_feat = log_feats[:, -1, :]\n",
        "\n",
        "        # 3. Dot Product similarity ê³„ì‚°\n",
        "        # result: (Batch, Item_Count + 1) -> each item's recommendation score\n",
        "        logits = torch.matmul(final_feat, item_embs.t())\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "BMd8qrHLTDUC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í•™ìŠµ ë£¨í”„ ì •ì˜\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for _, input_seq, target_pos, _ in dataloader:\n",
        "        input_seq = input_seq.to(device)\n",
        "        targets = target_pos[:, -1].to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        log_feats = model(input_seq)\n",
        "        logits = model.predict(log_feats)\n",
        "\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# validation í•¨ìˆ˜ ì •ì˜\n",
        "def validate_performance(model, user_train, user_valid, device, k=10):\n",
        "    model.eval()\n",
        "    hits = 0\n",
        "    with torch.no_grad():\n",
        "        for user, target_list in user_valid.items():\n",
        "            target = target_list[0]\n",
        "            # ìµœê·¼ ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°\n",
        "            seq = user_train[user][-max_len:]\n",
        "            input_seq = np.zeros([max_len], dtype=np.int32)\n",
        "            input_seq[-len(seq):] = seq\n",
        "            input_seq = torch.LongTensor([input_seq]).to(device)\n",
        "\n",
        "            logits = model.predict(model(input_seq))\n",
        "            _, top_items = torch.topk(logits, k)\n",
        "            if target in top_items[0]:\n",
        "                hits += 1\n",
        "    return hits / len(user_valid)"
      ],
      "metadata": {
        "id": "h3SI8TZKEgPL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SASRec í•™ìŠµ\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- [ì„¤ì •ê°’ ì •ì˜] ---\n",
        "item_count = len(unique_movies) + 1 #3884\n",
        "hidden_units = 128\n",
        "num_blocks = 2\n",
        "num_heads = 4\n",
        "max_len = 50\n",
        "dropout_rate = 0.2\n",
        "lr = 0.0005\n",
        "num_epochs = 100\n",
        "patience = 5  # ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ ê¸°ë‹¤ë¦´ íšŸìˆ˜\n",
        "counter = 0   # í˜„ì¬ ê¸°ë‹¤ë¦° íšŸìˆ˜\n",
        "best_val_hit = 0.0\n",
        "\n",
        "\n",
        "# --- [ì¤€ë¹„ ë‹¨ê³„] ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "# 1. ë°ì´í„°ë¡œë” ì¤€ë¹„\n",
        "train_dataset = SASRecDataset(user_train, item_count, max_len)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# 2. ëª¨ë¸ ì„ ì–¸\n",
        "model = SASRec(item_count, hidden_units, num_blocks, num_heads, max_len, dropout_rate, device).to(device)\n",
        "\n",
        "# 3. Loss function = CrossEntropyLoss / Optimizer = Adam\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # íŒ¨ë”©(0)ì€ í•™ìŠµì—ì„œ ì œì™¸\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98))\n",
        "\n",
        "\n",
        "# --- [í•™ìŠµ ì‹¤í–‰] ---\n",
        "\n",
        "print(f\"í•™ìŠµ ì‹œì‘! (Device: {device})\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # 1. í•™ìŠµ\n",
        "    avg_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "    # 2. ê²€ì¦\n",
        "    current_val_hit = validate_performance(model, user_train, user_valid, device, k=10)\n",
        "\n",
        "    print(f\"Epoch [{epoch}] - Loss: {avg_loss:.4f}, Val Hit@10: {current_val_hit:.4f}\")\n",
        "\n",
        "    # 3. Early Stopping ë¡œì§\n",
        "    if current_val_hit > best_val_hit:\n",
        "        best_val_hit = current_val_hit\n",
        "        counter = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ ì¹´ìš´í„° ì´ˆê¸°í™”\n",
        "        torch.save(model.state_dict(), 'best_sasrec_model.pth')\n",
        "        print(f\"  >> Best Model Saved! (Hit@10: {best_val_hit:.4f})\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"  >> No improvement. (Counter: {counter}/{patience})\")\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(f\"ğŸ›‘ Early Stopping triggered! {epoch} ì—í¬í¬ì—ì„œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
        "            break\n",
        "\n",
        "# í•™ìŠµ ì¢…ë£Œ í›„ ê°€ì¥ ì¢‹ì•˜ë˜ ëª¨ë¸ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "model.load_state_dict(torch.load('best_sasrec_model.pth'))\n",
        "print(\"ìµœì¢… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJnzqvN2ZLeR",
        "outputId": "4a90c1ab-09d0-4b17-942b-4a766ceda31e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "í•™ìŠµ ì‹œì‘! (Device: cpu)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2307019045.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  input_seq = torch.LongTensor([input_seq]).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0] - Loss: 32.9616, Val Hit@10: 0.0036\n",
            "  >> Best Model Saved! (Hit@10: 0.0036)\n",
            "Epoch [1] - Loss: 20.9181, Val Hit@10: 0.0109\n",
            "  >> Best Model Saved! (Hit@10: 0.0109)\n",
            "Epoch [2] - Loss: 16.4093, Val Hit@10: 0.0118\n",
            "  >> Best Model Saved! (Hit@10: 0.0118)\n",
            "Epoch [3] - Loss: 13.6643, Val Hit@10: 0.0154\n",
            "  >> Best Model Saved! (Hit@10: 0.0154)\n",
            "Epoch [4] - Loss: 11.7902, Val Hit@10: 0.0205\n",
            "  >> Best Model Saved! (Hit@10: 0.0205)\n",
            "Epoch [5] - Loss: 10.6662, Val Hit@10: 0.0265\n",
            "  >> Best Model Saved! (Hit@10: 0.0265)\n",
            "Epoch [6] - Loss: 9.8013, Val Hit@10: 0.0270\n",
            "  >> Best Model Saved! (Hit@10: 0.0270)\n",
            "Epoch [7] - Loss: 9.1783, Val Hit@10: 0.0242\n",
            "  >> No improvement. (Counter: 1/5)\n",
            "Epoch [8] - Loss: 8.7392, Val Hit@10: 0.0233\n",
            "  >> No improvement. (Counter: 2/5)\n",
            "Epoch [9] - Loss: 8.3981, Val Hit@10: 0.0227\n",
            "  >> No improvement. (Counter: 3/5)\n",
            "Epoch [10] - Loss: 8.1176, Val Hit@10: 0.0220\n",
            "  >> No improvement. (Counter: 4/5)\n",
            "Epoch [11] - Loss: 7.9078, Val Hit@10: 0.0245\n",
            "  >> No improvement. (Counter: 5/5)\n",
            "ğŸ›‘ Early Stopping triggered! 11 ì—í¬í¬ì—ì„œ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n",
            "ìµœì¢… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ í™•ì¸ìš© í‰ê°€(1:99 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ë°©ë²•. SASRec ë…¼ë¬¸ ê¸°ì¤€ NDCG@10 = 0.5905)\n",
        "\n",
        "import random\n",
        "\n",
        "def evaluate_1_99(model, user_train, user_test, item_count, max_len, device, k=10):\n",
        "    model.eval()\n",
        "    recalls = []\n",
        "    ndcgs = []\n",
        "\n",
        "    # ì „ì²´ ì•„ì´í…œ ì…‹ (ìƒ˜í”Œë§ìš©)\n",
        "    all_items = set(range(1, item_count))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user in user_test.keys():\n",
        "            # 1. ì •ë‹µ ì•„ì´í…œ ë° ìœ ì €ê°€ ë³¸ ì•„ì´í…œ ì œì™¸ í›„ë³´ ìƒì„±\n",
        "            target = user_test[user][0]\n",
        "            rated_items = set(user_train[user])\n",
        "            if user in user_test:\n",
        "                rated_items.update(user_test[user])\n",
        "\n",
        "            # 2. 99ê°œì˜ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§\n",
        "            negative_samples = []\n",
        "            while len(negative_samples) < 99:\n",
        "                sampled_item = random.randint(1, item_count - 1)\n",
        "                if sampled_item not in rated_items:\n",
        "                    negative_samples.append(sampled_item)\n",
        "\n",
        "            # 3. í‰ê°€ ëŒ€ìƒ 100ê°œ (ì •ë‹µ 1 + ì˜¤ë‹µ 99)\n",
        "            test_items = [target] + negative_samples\n",
        "\n",
        "            # 4. ëª¨ë¸ ì˜ˆì¸¡\n",
        "            seq = user_train[user][-max_len:]\n",
        "            input_seq = np.zeros([max_len], dtype=np.int32)\n",
        "            input_seq[-len(seq):] = seq\n",
        "            input_seq = torch.LongTensor([input_seq]).to(device)\n",
        "\n",
        "            log_feats = model(input_seq)\n",
        "            logits = model.predict(log_feats) # (1, Item_Count)\n",
        "\n",
        "            # 5. 100ê°œ ì•„ì´í…œì— ëŒ€í•œ ì ìˆ˜ë§Œ ì¶”ì¶œ\n",
        "            test_logits = logits[0, test_items] # (100,)\n",
        "\n",
        "            # 6. Rank ê³„ì‚° (ê°’ì´ í´ìˆ˜ë¡ ë†’ì€ ìˆœìœ„)\n",
        "            # test_logits[0]ê°€ ì •ë‹µ(target)ì˜ ì ìˆ˜ì„\n",
        "            target_score = test_logits[0]\n",
        "            # ì •ë‹µë³´ë‹¤ ì ìˆ˜ê°€ ë†’ì€ ì•„ì´í…œì˜ ê°œìˆ˜ë¥¼ ì„¸ì–´ ìˆœìœ„ ê²°ì •\n",
        "            rank = (test_logits > target_score).sum().item() + 1\n",
        "\n",
        "            # 7. Metrics ê³„ì‚°\n",
        "            # Recall@K\n",
        "            if rank <= k:\n",
        "                recalls.append(1)\n",
        "                # NDCG@K\n",
        "                ndcgs.append(1 / np.log2(rank + 1))\n",
        "            else:\n",
        "                recalls.append(0)\n",
        "                ndcgs.append(0)\n",
        "\n",
        "    avg_recall = np.mean(recalls)\n",
        "    avg_ndcg = np.mean(ndcgs)\n",
        "\n",
        "    print(f\"--- 1:99 Sampling Evaluation (K={k}) ---\")\n",
        "    print(f\"Recall@{k}: {avg_recall:.4f}\")\n",
        "    print(f\"NDCG@{k}: {avg_ndcg:.4f}\") # IDCGëŠ” 1ì´ë¯€ë¡œ DCG ìì²´ê°€ NDCG\n",
        "\n",
        "    return avg_recall, avg_ndcg\n",
        "\n",
        "# ì‹¤í–‰\n",
        "recall_1_99, ndcg_1_99 = evaluate_1_99(model, user_train, user_test, item_count, max_len, device, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QbNkBw4LLDX",
        "outputId": "0b560d75-4d3e-4288-e834-33703548558c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1:99 Sampling Evaluation (K=10) ---\n",
            "Recall@10: 0.3699\n",
            "NDCG@10: 0.2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‹¤í—˜ìš© ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥. ìƒ˜í”Œë§ ì—†ì´ ì•„ì´í…œ ì „ì²´ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•¨.\n",
        "\n",
        "def evaluate_model(model, user_train, user_test, item_count, max_len, device, k=10):\n",
        "    model.eval()\n",
        "    recalls = []\n",
        "    ndcgs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user in user_test.keys():\n",
        "            # ìœ ì €ì˜ í•™ìŠµ ì‹œí€€ìŠ¤ ì¤€ë¹„\n",
        "            seq = user_train[user][-max_len:]\n",
        "            input_seq = np.zeros([max_len], dtype=np.int32)\n",
        "            input_seq[-len(seq):] = seq\n",
        "            input_seq = torch.LongTensor([input_seq]).to(device)\n",
        "\n",
        "            # ì˜ˆì¸¡\n",
        "            log_feats = model(input_seq)\n",
        "            logits = model.predict(log_feats) # (1, Item_Count)\n",
        "\n",
        "            # ì •ë‹µ ì•„ì´í…œ\n",
        "            target = user_test[user][0]\n",
        "\n",
        "            # í‰ê°€: evaluate_metrics í•¨ìˆ˜ í™œìš©\n",
        "            metrics = evaluate_metrics(logits, torch.LongTensor([target]), k_list=[k])\n",
        "            recalls.append(metrics[f'Recall@{k}'])\n",
        "            ndcgs.append(metrics[f'NDCG@{k}'])\n",
        "\n",
        "    print(f\"í‰ê°€ ì™„ë£Œ (K={k}) -> Recall: {np.mean(recalls):.4f}, NDCG: {np.mean(ndcgs):.4f}\")\n",
        "    return np.mean(recalls), np.mean(ndcgs)\n",
        "\n",
        "# ì‹¤í–‰\n",
        "evaluate_model(model, user_train, user_test, item_count, max_len, device, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyBVEvTkzEsN",
        "outputId": "565dd984-540a-4f6b-b2cb-0f4972d9c021"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í‰ê°€ ì™„ë£Œ (K=10) -> Recall: 0.0331, NDCG: 0.0156\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.033112582781456956), np.float64(0.015639827976732776))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_long_term_metrics(model, user_train, user_test, item_genre_dict, all_item_counts, total_users, transition_matrix, genre_to_idx, device, k=10):\n",
        "    model.eval()\n",
        "    all_metrics = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_users = list(user_test.keys())\n",
        "\n",
        "        for user in test_users:\n",
        "            seq = user_train[user][-max_len:]\n",
        "            input_seq = np.zeros([max_len], dtype=np.int32)\n",
        "            input_seq[-len(seq):] = seq\n",
        "            input_seq = torch.LongTensor([input_seq]).to(device)\n",
        "\n",
        "            logits = model.predict(model(input_seq))\n",
        "            # ë³¸ ì˜í™” ì œì™¸í•˜ê³  Top-K ì¶”ì¶œ\n",
        "            logits[0, user_train[user]] = -1e9\n",
        "            _, top_items = torch.topk(logits, k)\n",
        "            recommended_list = top_items[0].cpu().numpy().tolist()\n",
        "\n",
        "            # ì¥ê¸° ì§€í‘œ í•¨ìˆ˜ í˜¸ì¶œ\n",
        "            metrics = evaluate_long_term_metrics(\n",
        "                recommended_list, user_train[user], user_test[user][0],\n",
        "                item_genre_dict, all_item_counts, total_users, transition_matrix, genre_to_idx\n",
        "            )\n",
        "            all_metrics.append(metrics)\n",
        "\n",
        "    # í‰ê·  ê³„ì‚°\n",
        "    avg_metrics = {m: np.mean([x[m] for x in all_metrics]) for m in all_metrics[0].keys()}\n",
        "    for k, v in avg_metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    return avg_metrics\n",
        "\n",
        "# ì‹¤í–‰\n",
        "get_long_term_metrics(model, user_train, user_test, item_genre_dict, all_item_counts, total_users, transition_matrix, genre_to_idx, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gzSmNvZzFqn",
        "outputId": "7edcfe2c-bf4d-4861-bfee-da27c13ca7e6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diversity: 0.8587\n",
            "Novelty: 2.4329\n",
            "Serendipity: 0.0333\n",
            "Transition_Prob: 0.1007\n",
            "Recency_Satisfy: 14.2875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Diversity': np.float64(0.8586648285387248),\n",
              " 'Novelty': np.float64(2.432867438437269),\n",
              " 'Serendipity': np.float64(0.03327667813500524),\n",
              " 'Transition_Prob': np.float64(0.10073262244997451),\n",
              " 'Recency_Satisfy': np.float64(14.287512498091713)}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}