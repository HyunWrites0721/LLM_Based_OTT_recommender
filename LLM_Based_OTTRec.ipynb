{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1b16qeSkKvp0H6sBVJr1Ad4YuaFRueFOC",
      "authorship_tag": "ABX9TyPuZOVga0Id8KmYpbABjP5x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyunWrites0721/LLM_Based_OTT_recommender/blob/main/LLM_Based_OTTRec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "GTM_LJdQuDDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7014fb7b-746e-43b4-f30f-8dc3f0027768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 로드 완료!\n",
            "유저 수: 6040, 영화 수: 3883, 평점 수: 1000209\n"
          ]
        }
      ],
      "source": [
        "#데이터 로딩.\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/for_data/ml-1m/\"\n",
        "\n",
        "# 1. Ratings: 유저의 영화 평점 데이터\n",
        "ratings = pd.read_csv(path + 'ratings.dat', sep='::', engine='python', encoding='latin-1',\n",
        "                      names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
        "\n",
        "# 2. Movies: 영화 제목 및 장르 데이터\n",
        "movies = pd.read_csv(path + 'movies.dat', sep='::', engine='python', encoding='latin-1',\n",
        "                     names=['movie_id', 'title', 'genres'])\n",
        "\n",
        "# 3. Users: 유저 데모그래픽(성별, 나이 등) 데이터\n",
        "users = pd.read_csv(path + 'users.dat', sep='::', engine='python', encoding='latin-1',\n",
        "                    names=['user_id', 'gender', 'age', 'occupation', 'zip_code'])\n",
        "\n",
        "print(\"데이터 로드 완료!\")\n",
        "print(f\"유저 수: {len(users)}, 영화 수: {len(movies)}, 평점 수: {len(ratings)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 정제\n",
        "\n",
        "# 1. 제목(Title)을 기준으로 고유한 영화 목록 생성\n",
        "unique_movies = movies.drop_duplicates(subset=['title']).copy()\n",
        "\n",
        "# 2. 새로운 고유 ID 부여 (1부터 시작, 패딩이 0이므로 1부터 시작해야 함.)\n",
        "unique_movies['new_movie_id'] = range(1, len(unique_movies)+1)\n",
        "\n",
        "# 3. {기존 ID : 새로운 ID} 매핑 딕셔너리 생성\n",
        "id_mapping = pd.merge(movies, unique_movies[['title', 'new_movie_id']], on='title')\n",
        "mapping_dict = dict(zip(id_mapping['movie_id'], id_mapping['new_movie_id']))\n",
        "\n",
        "# 4. Ratings 데이터 기존 ID를 새로운 고유 ID로 치환 (매핑되지 않는 유령 ID는 제거)\n",
        "ratings['movie_id'] = ratings['movie_id'].map(mapping_dict)\n",
        "ratings = ratings.dropna(subset=['movie_id']).astype({'movie_id': int})\n",
        "\n",
        "# 5. 최종 결과 확인\n",
        "print(f\"기존 영화 수: {len(movies)}\")\n",
        "print(f\"정제 후 고유 영화 수: {len(unique_movies)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywc5RUGvviSc",
        "outputId": "e491d203-48a5-49dc-936c-a5a1d2300c57"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 영화 수: 3883\n",
            "정제 후 고유 영화 수: 3883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(unique_movies)\n",
        "print(ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NECCwL9C8ccX",
        "outputId": "e6e0e8f3-edce-4246-ce40-26e2fde238d0"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      movie_id                               title  \\\n",
            "0            1                    Toy Story (1995)   \n",
            "1            2                      Jumanji (1995)   \n",
            "2            3             Grumpier Old Men (1995)   \n",
            "3            4            Waiting to Exhale (1995)   \n",
            "4            5  Father of the Bride Part II (1995)   \n",
            "...        ...                                 ...   \n",
            "3878      3948             Meet the Parents (2000)   \n",
            "3879      3949          Requiem for a Dream (2000)   \n",
            "3880      3950                    Tigerland (2000)   \n",
            "3881      3951             Two Family House (2000)   \n",
            "3882      3952               Contender, The (2000)   \n",
            "\n",
            "                            genres  new_movie_id  \n",
            "0      Animation|Children's|Comedy             1  \n",
            "1     Adventure|Children's|Fantasy             2  \n",
            "2                   Comedy|Romance             3  \n",
            "3                     Comedy|Drama             4  \n",
            "4                           Comedy             5  \n",
            "...                            ...           ...  \n",
            "3878                        Comedy          3879  \n",
            "3879                         Drama          3880  \n",
            "3880                         Drama          3881  \n",
            "3881                         Drama          3882  \n",
            "3882                Drama|Thriller          3883  \n",
            "\n",
            "[3883 rows x 4 columns]\n",
            "         user_id  movie_id  rating  timestamp\n",
            "0              1      1177       5  978300760\n",
            "1              1       656       3  978302109\n",
            "2              1       903       3  978301968\n",
            "3              1      3340       4  978300275\n",
            "4              1      2287       5  978824291\n",
            "...          ...       ...     ...        ...\n",
            "1000204     6040      1076       1  956716541\n",
            "1000205     6040      1079       5  956704887\n",
            "1000206     6040       559       5  956704746\n",
            "1000207     6040      1081       4  956715648\n",
            "1000208     6040      1082       4  956715569\n",
            "\n",
            "[1000209 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ratings에 바뀐 ID들이 잘 들어갔는지 확인\n",
        "print(f\"최대 영화 ID: {ratings['movie_id'].max()}\") # 3883이 나와야 함\n",
        "print(f\"최소 영화 ID: {ratings['movie_id'].min()}\") # 1이 나와야 함\n",
        "\n",
        "# 2. item_count 재설정\n",
        "item_count = ratings['movie_id'].max() + 1 # 3884가 되어야 함 (0번 패딩 포함)\n",
        "\n",
        "item_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjOsAR4bm1pz",
        "outputId": "0743999f-1ba5-4215-dcd4-17717c47e96b"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 영화 ID: 3883\n",
            "최소 영화 ID: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3884"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SASRec 학습용 유저 시퀀스 만들기\n",
        "\n",
        "# 유저별로 영화 시청 이력을 시간순으로 묶기\n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "user_test = {}\n",
        "\n",
        "user_group = ratings.groupby('user_id')\n",
        "#print(user_group.groups)\n",
        "\n",
        "for user, group in user_group:\n",
        "    seq = group.sort_values('timestamp')['movie_id'].tolist()\n",
        "\n",
        "    # 데이터가 너무 적은 유저는 제외 (최소 3개 이상, train/valid/test로 분할하기 위해)\n",
        "    if len(seq) < 3:\n",
        "        continue\n",
        "\n",
        "    # Leave-one-out 분할\n",
        "    user_train[user] = seq[:-2]\n",
        "    user_valid[user] = [seq[-2]]\n",
        "    user_test[user] = [seq[-1]]\n",
        "\n",
        "print(f\"남은 유저 수: {len(user_train)}\")\n",
        "print(f\"1번 유저의 학습 시퀀스 예시: {user_train[1][:5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8IwFixvAjpR",
        "outputId": "fa0cb340-94c9-4c02-e2b7-43c55cc2f0e1"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "남은 유저 수: 6040\n",
            "1번 유저의 학습 시퀀스 예시: [3118, 1251, 1673, 1010, 2272]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#평가용 메타데이터1 : total_users, all_item_counts, item_genre_dict\n",
        "\n",
        "# 1. total_users: 전체 유저 수 계산\n",
        "total_users = ratings['user_id'].nunique()\n",
        "\n",
        "# 2. all_item_counts: 아이템별 노출 빈도 (Novelty 계산용)\n",
        "# 각 영화가 몇 명의 유저에게 소비되었는지 계산\n",
        "all_item_counts = ratings['movie_id'].value_counts().to_dict()\n",
        "\n",
        "# 3. item_genre_dict: {new_movie_id: [장르 리스트]} (Diversity, Transition용)\n",
        "item_genre_dict = {}\n",
        "\n",
        "for _, row in unique_movies.iterrows():\n",
        "    new_id = row['new_movie_id']\n",
        "    # 장르 데이터가 'Action|Sci-Fi' 형태일 경우 리스트로 분할\n",
        "    genres = row['genres'].split('|')\n",
        "    item_genre_dict[new_id] = genres\n",
        "\n",
        "#print(f\"전체 유저 수: {total_users}\")\n",
        "#print(f\"장르 정보가 포함된 영화 수: {len(item_genre_dict)}\")\n",
        "#print(f\"1번 영화의 장르: {item_genre_dict.get(1)}\")\n",
        "\n",
        "\n",
        "#평가용 메타데이터2 : all_genres, genre_to_idx, idx_to_genre\n",
        "\n",
        "# 모든 고유 장르 추출 및 정렬\n",
        "all_genres = sorted(list(set([g for genres in item_genre_dict.values() for g in genres])))\n",
        "\n",
        "# 장르명 <-> 행렬 인덱스 매핑\n",
        "genre_to_idx = {g: i for i, g in enumerate(all_genres)}\n",
        "idx_to_genre = {i: g for i, g in enumerate(all_genres)}\n",
        "\n",
        "#print(f\"추출된 총 장르 수: {len(all_genres)}\")\n",
        "#print(f\"장르 매핑 예시: {list(genre_to_idx.items())[:3]}\")"
      ],
      "metadata": {
        "id": "MwYWttBLr4r3"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#평가용 메타데이터3: 장르 전이 행렬 계산 함수: 한 장르에서 다른 장르로 넘어갈 확률을 저장하는 행렬\n",
        "\n",
        "def build_genre_transition_matrix(ratings, item_genre_dict):\n",
        "    # 모든 고유 장르 추출\n",
        "    all_genres = sorted(list(set([g for genres in item_genre_dict.values() for g in genres])))\n",
        "    genre_to_idx = {g: i for i, g in enumerate(all_genres)}\n",
        "    num_genres = len(all_genres)\n",
        "\n",
        "    # matrix[from_genre][to_genre]\n",
        "    matrix = np.zeros((num_genres, num_genres))\n",
        "\n",
        "    # 유저별 시퀀스 확인\n",
        "    for user, group in ratings.groupby('user_id'):\n",
        "        seq = group.sort_values('timestamp')['movie_id'].tolist()\n",
        "        for i in range(len(seq) - 1):\n",
        "            from_genres = item_genre_dict.get(seq[i], [])\n",
        "            to_genres = item_genre_dict.get(seq[i+1], [])\n",
        "\n",
        "            # 다중 장르일 경우 모든 조합에 대해 빈도 추가 (분산 처리)\n",
        "            for fg in from_genres:\n",
        "                for tg in to_genres:\n",
        "                    matrix[genre_to_idx[fg]][genre_to_idx[tg]] += 1\n",
        "\n",
        "    # 행 단위로 정규화 (확률로 변환)\n",
        "    row_sums = matrix.sum(axis=1)\n",
        "    # 0으로 나누기 방지\n",
        "    transition_matrix = np.divide(matrix, row_sums[:, np.newaxis],\n",
        "                                  where=row_sums[:, np.newaxis]!=0)\n",
        "\n",
        "    return transition_matrix, genre_to_idx\n",
        "\n",
        "# 실행\n",
        "transition_matrix, genre_mapping = build_genre_transition_matrix(ratings, item_genre_dict)"
      ],
      "metadata": {
        "id": "lFY6XjuhoAQF"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SASRecDataset 클래스 정의\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class SASRecDataset(Dataset):\n",
        "    def __init__(self, user_train, item_count, max_len):\n",
        "        self.user_train = user_train  # {user ID: [item list]} dictionary\n",
        "        self.item_count = item_count  # total number of movies\n",
        "        self.max_len = max_len        # maximum sequence length L\n",
        "        self.user_list = list(user_train.keys())  # List of user ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.user_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        user = self.user_list[index]\n",
        "        seq = self.user_train[user]\n",
        "\n",
        "        # 1. Sequence Padding & Truncating\n",
        "        # Pad by 0 or Truncate if larger than L\n",
        "        input_seq = np.zeros([self.max_len], dtype=np.int32)\n",
        "        pos = np.zeros([self.max_len], dtype=np.int32)  #true next item\n",
        "        neg = np.zeros([self.max_len], dtype=np.int32)  #not next item\n",
        "\n",
        "        # nxt는 정답(Positive), idx는 현재 입력(Input)\n",
        "        nxt = seq[-1]\n",
        "        idx = self.max_len - 1\n",
        "\n",
        "        # 뒤에서부터 채워나가는 방식 (Left Padding을 위함)\n",
        "        for i in reversed(seq[:-1]):\n",
        "            input_seq[idx] = i\n",
        "            pos[idx] = nxt\n",
        "            # Negative Sampling: random sampling from items user haven't seen\n",
        "            if nxt != 0: # if not padding\n",
        "                neg[idx] = self._get_neg_item(seq)\n",
        "            nxt = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "\n",
        "        return torch.LongTensor([user]), torch.LongTensor(input_seq), \\\n",
        "               torch.LongTensor(pos), torch.LongTensor(neg)\n",
        "\n",
        "    def _get_neg_item(self, seq):\n",
        "        # 학습 효율을 위해 유저 시퀀스에 없는 아이템을 랜덤하게 뽑음\n",
        "        t = np.random.randint(1, self.item_count)\n",
        "        while t in seq:\n",
        "            t = np.random.randint(1, self.item_count)\n",
        "        return t"
      ],
      "metadata": {
        "id": "Y-V4sNWxnWMi"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#평가용 단기지표 계산 함수\n",
        "#반환값: Recall, NDCG\n",
        "\n",
        "import math\n",
        "\n",
        "def evaluate_metrics(predictions, ground_truth, k_list=[5, 10]):\n",
        "    \"\"\"\n",
        "    predictions: 모델이 예측한 전체 아이템에 대한 점수 (Batch, Item_Count)\n",
        "    ground_truth: 실제 정답 아이템 ID (Batch, 1)\n",
        "    k_list: 계산하고 싶은 K 값의 리스트\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # 1. 상위 K개의 아이템 인덱스(ID) 추출\n",
        "    # torch.topk는 내림차순으로 가장 높은 값 k개를 반환함\n",
        "    max_k = max(k_list)\n",
        "    _, top_indices = torch.topk(predictions, max_k, dim=-1) # (Batch, max_k)\n",
        "\n",
        "    # 2. 각 K별로 지표 계산\n",
        "    for k in k_list:\n",
        "        hit = 0.0\n",
        "        ndcg = 0.0\n",
        "\n",
        "        for i in range(len(ground_truth)):\n",
        "            target = ground_truth[i]       # 실제 정답 ID\n",
        "            recommended = top_indices[i][:k] # 상위 k개 추천 리스트\n",
        "\n",
        "            if target in recommended:\n",
        "                # [Recall 계산] 포함되면 1 아니면 0\n",
        "                hit += 1.0\n",
        "\n",
        "                # [NDCG 계산] 정답의 순위(rank)를 찾아 로그 감쇠 적용\n",
        "                # index는 0부터 시작하므로 +1을 해줌, 또 +1을 하는 이유는 log 때문에. 그래서 +2.\n",
        "                rank = (recommended == target).nonzero(as_tuple=True)[0].item()\n",
        "                ndcg += 1.0 / math.log2(rank + 2)\n",
        "\n",
        "        results[f'Recall@{k}'] = hit / len(ground_truth)\n",
        "        results[f'NDCG@{k}'] = ndcg / len(ground_truth)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ntEbcZ90rh5u"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#평가용 장기지표 계산 함수\n",
        "#Intra-List Diversity / Novelty / Serendipity / Genre-Transition Probability / Recency-Weighted Satisfaction\n",
        "\n",
        "def evaluate_long_term_metrics(recommended_list, user_history, ground_truth, item_genre_dict,\n",
        "                               all_item_counts, total_users, transition_matrix, genre_to_idx):\n",
        "    \"\"\"\n",
        "    ground_truth: 실제 유저가 본 다음 영화 ID (보통 user_test[user][0])\n",
        "    transition_matrix: build_genre_transition_matrix 함수로 만든 행렬\n",
        "    genre_to_idx: 장르명 -> 행렬 인덱스 매핑 사전\n",
        "    \"\"\"\n",
        "\n",
        "    # 공통 함수: Jaccard 유사도\n",
        "    def get_genre_sim(item1, item2):\n",
        "        g1, g2 = set(item_genre_dict.get(item1, [])), set(item_genre_dict.get(item2, []))\n",
        "        if not g1 or not g2: return 0\n",
        "        return len(g1 & g2) / len(g1 | g2)\n",
        "\n",
        "    # 1. Intra-List Diversity\n",
        "    sims = []\n",
        "    for i in range(len(recommended_list)):\n",
        "        for j in range(i + 1, len(recommended_list)):\n",
        "            sims.append(get_genre_sim(recommended_list[i], recommended_list[j]))\n",
        "    intra_diversity = 1 - np.mean(sims) if sims else 0\n",
        "\n",
        "    # 2. Novelty\n",
        "    novelty = 0\n",
        "    for item in recommended_list:\n",
        "        prob = (all_item_counts.get(item, 0)+1) / (total_users + 1)\n",
        "        novelty -= np.log2(prob)\n",
        "    novelty /= len(recommended_list)\n",
        "\n",
        "    # 3. Serendipity\n",
        "    # 추천 리스트에 정답(ground_truth)이 포함되어 있어야 하며, 그 정답이 과거 히스토리 아이템들과의 평균 유사도가 낮아야 함.\n",
        "    serendipity = 0\n",
        "    if ground_truth in recommended_list:\n",
        "        # 정답 아이템과 과거 히스토리 간의 평균 유사도 계산\n",
        "        rel_sims = [get_genre_sim(ground_truth, hist_item) for hist_item in user_history]\n",
        "        avg_rel_sim = np.mean(rel_sims) if rel_sims else 0\n",
        "        # 유사도가 낮을수록(즉, 의외일수록) 높은 점수\n",
        "        serendipity = 1 - avg_rel_sim\n",
        "\n",
        "    # 4. Genre-Transition Probability (전이 행렬 활용)\n",
        "    # 마지막 아이템의 장르들로부터 추천 리스트 아이템들의 장르들로 이동할 확률의 평균\n",
        "    from_genres = item_genre_dict.get(user_history[-1], [])\n",
        "    transition_scores = []\n",
        "\n",
        "    for rec_item in recommended_list:\n",
        "        to_genres = item_genre_dict.get(rec_item, [])\n",
        "        item_transition_probs = []\n",
        "        for fg in from_genres:\n",
        "            for tg in to_genres:\n",
        "                # 행렬에서 확률값 추출\n",
        "                f_idx, t_idx = genre_to_idx[fg], genre_to_idx[tg]\n",
        "                item_transition_probs.append(transition_matrix[f_idx, t_idx])\n",
        "\n",
        "        if item_transition_probs:\n",
        "            transition_scores.append(np.mean(item_transition_probs))\n",
        "\n",
        "    genre_transition = np.mean(transition_scores) if transition_scores else 0\n",
        "\n",
        "    # 5. Recency-Weighted Satisfaction\n",
        "    recency_weighted_sim = 0\n",
        "    lambda_decay = 0.1\n",
        "    for idx, hist_item in enumerate(reversed(user_history)):\n",
        "        weight = np.exp(-lambda_decay * idx)\n",
        "        for rec_item in recommended_list:\n",
        "            recency_weighted_sim += weight * get_genre_sim(hist_item, rec_item)\n",
        "\n",
        "    return {\n",
        "        \"Diversity\": intra_diversity,\n",
        "        \"Novelty\": novelty,\n",
        "        \"Serendipity\": serendipity,\n",
        "        \"Transition_Prob\": genre_transition,\n",
        "        \"Recency_Satisfy\": recency_weighted_sim\n",
        "    }"
      ],
      "metadata": {
        "id": "gh4iedTSkMJ_"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 구조 정의 : class SASRec(nn.Module)\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "    def __init__(self, item_count, hidden_units, num_blocks, num_heads, max_len, dropout_rate, device):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.item_count = item_count\n",
        "        self.device = device\n",
        "\n",
        "        # 1. Embedding Layers\n",
        "        # item_count + 1 인 이유는 0번 패딩 때문\n",
        "        self.item_emb = nn.Embedding(item_count, hidden_units, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding(max_len + 1, hidden_units)\n",
        "        self.emb_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # 2. Multi-head Attention Blocks\n",
        "        self.attention_blocks = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=hidden_units,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=hidden_units,\n",
        "                dropout=dropout_rate,\n",
        "                activation='relu',\n",
        "                batch_first=True\n",
        "            ) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.last_layernorm = nn.LayerNorm(hidden_units)\n",
        "\n",
        "    def forward(self, log_seqs):\n",
        "        # log_seqs: (Batch, max_len)\n",
        "\n",
        "        # 아이템 임베딩 + 위치 임베딩\n",
        "        seqs = self.item_emb(log_seqs) # (Batch, max_len, hidden_units) --> 아이템 하나당 hidden_units만큼의 임베딩 차원을 지님.\n",
        "        positions = torch.arange(log_seqs.shape[1], device=self.device).unsqueeze(0)\n",
        "        seqs += self.pos_emb(positions)\n",
        "        seqs = self.emb_dropout(seqs)\n",
        "\n",
        "        # 0번 패딩 마스크 생성\n",
        "        timeline_mask = (log_seqs == 0) # (Batch, max_len)\n",
        "\n",
        "        # Causal Mask (미래 정보 막기)\n",
        "        # nn.TransformerEncoderLayer에서 쓸 수 있도록 상삼각행렬 생성. 상삼각 부분에만 -inf를 취함으로써 마스킹.\n",
        "        # attn_mask = torch.triu(torch.ones((log_seqs.shape[1], log_seqs.shape[1]), device=self.device), diagonal=1).bool()\n",
        "        attn_mask = torch.triu(torch.ones((log_seqs.shape[1], log_seqs.shape[1]), device=self.device), diagonal=1)\n",
        "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).to(torch.float32)\n",
        "\n",
        "        # Attention Blocks 통과\n",
        "        for block in self.attention_blocks:\n",
        "            # src_key_padding_mask와 mask를 사용하여 패딩과 미래 정보를 차단\n",
        "            seqs = block(seqs, src_mask=attn_mask, src_key_padding_mask=timeline_mask)\n",
        "\n",
        "        log_feats = self.last_layernorm(seqs) # (Batch, max_len, hidden_units)\n",
        "\n",
        "        return log_feats\n",
        "\n",
        "    def predict(self, log_feats):\n",
        "        \"\"\"\n",
        "        log_feats: (Batch, Max_Len, Hidden_Units) - model's result(recommendation)\n",
        "        \"\"\"\n",
        "        # 1. 모든 아이템 임베딩을 가져옴 (Item_Count + 1, Hidden_Units)\n",
        "        item_embs = self.item_emb.weight\n",
        "\n",
        "        # 2. 유저의 마지막 상태(취향)을 추출 (Batch, Hidden_Units)\n",
        "        # log_feats's last time step [-1] is user's current(final) taste\n",
        "        final_feat = log_feats[:, -1, :]\n",
        "\n",
        "        # 3. Dot Product similarity 계산\n",
        "        # result: (Batch, Item_Count + 1) -> each item's recommendation score\n",
        "        logits = torch.matmul(final_feat, item_embs.t())\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "BMd8qrHLTDUC"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 루프 정의\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train() # 학습 모드 시작 (Dropout 활성화)\n",
        "    total_loss = 0\n",
        "\n",
        "    for user_ids, input_seq, target_pos, _ in dataloader:\n",
        "        # Transport data to GPU/CPU\n",
        "        input_seq = input_seq.to(device)\n",
        "        target_pos = target_pos[:, -1].to(device) #마지막 아이템 (Batch)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad() # gradient 초기화\n",
        "        log_feats = model(input_seq)\n",
        "        logits = model.predict(log_feats) # make scores for all items\n",
        "\n",
        "        # Loss\n",
        "        # logits: (Batch, Item_Count+1), target_pos: (Batch)\n",
        "        loss = criterion(logits, target_pos)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward() # calculate jacobians\n",
        "        optimizer.step() # update weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "h3SI8TZKEgPL"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SASRec 학습\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- [설정값 정의] ---\n",
        "item_count = len(unique_movies) + 1 #3884\n",
        "hidden_units = 64\n",
        "num_blocks = 2\n",
        "num_heads = 1\n",
        "max_len = 50\n",
        "dropout_rate = 0.2\n",
        "lr = 0.001\n",
        "num_epochs = 20\n",
        "\n",
        "\n",
        "# --- [준비 단계] ---\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "# 1. 데이터로더 준비\n",
        "train_dataset = SASRecDataset(user_train, item_count, max_len)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# 2. 모델 선언\n",
        "model = SASRec(item_count, hidden_units, num_blocks, num_heads, max_len, dropout_rate, device).to(device)\n",
        "\n",
        "# 3. Loss function = CrossEntropyLoss / Optimizer = Adam\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # 패딩(0)은 학습에서 제외\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98))\n",
        "\n",
        "\n",
        "# --- [학습 실행] ---\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    avg_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"학습 완료!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HJnzqvN2ZLeR",
        "outputId": "f2236a36-42bc-48cc-817a-a79206f53106"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3315499254.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch}/{num_epochs}] - Loss: {avg_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4124119738.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calculate jacobians\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_graph_capture_health_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         ):\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0mcapturing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_current_stream_capturing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             if capturing and not all(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/graphs.py\u001b[0m in \u001b[0;36mis_current_stream_capturing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mIf\u001b[0m \u001b[0ma\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexist\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0mwithout\u001b[0m \u001b[0minitializing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_cuda_isCurrentStreamCapturing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ]
}